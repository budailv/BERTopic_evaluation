{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5784bd3-6e9b-4698-9cc9-ef3741c4c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import DataLoader, Trainer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dce67-cef1-4c3e-abc8-a70671b43b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d09fac-23e9-4650-8339-3f95f7372d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46693/46693 [00:00<00:00, 561175.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "53637\n",
      "words filtering done\n",
      "CPU times: user 1.84 s, sys: 260 ms, total: 2.1 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader_trump = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630f335b-8949-4107-9300-c3b3e9933171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "dataset, custom = \"trump\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "#model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "#embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42243fab-8d6a-4cfb-a7a5-bf9d85a3b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"BERTopic_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ed93cc-38d8-45bc-8777-ae8f52e98e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:12:01,314 - top2vec - INFO - Pre-processing documents for training\n",
      "2024-03-02 15:12:03,171 - top2vec - INFO - Creating joint document/word embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnr_topics\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)],\n\u001b[1;32m      4\u001b[0m           \u001b[38;5;66;03m# \"embedding_model\": \"all-mpnet-base-v2\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdbscan_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_cluster_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      6\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_selection_method\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meom\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     10\u001b[0m                   custom_dataset\u001b[38;5;241m=\u001b[39mcustom,\n\u001b[1;32m     11\u001b[0m                   custom_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                   model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                   params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     14\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTop2Vec_trump_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BERTopic_evaluation/evaluation/evaluation.py:164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_combo \u001b[38;5;129;01min\u001b[39;00m new_params:\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     params_to_use \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    162\u001b[0m         param: value \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_name, param_combo)\n\u001b[1;32m    163\u001b[0m     }\n\u001b[0;32m--> 164\u001b[0m     output, computation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_tm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(output)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Update results\u001b[39;00m\n",
      "File \u001b[0;32m~/BERTopic_evaluation/evaluation/evaluation.py:207\u001b[0m, in \u001b[0;36mTrainer._train_tm_model\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Train Top2Vec\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_top2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Train LDAseq\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLDAseq\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/BERTopic_evaluation/evaluation/evaluation.py:275\u001b[0m, in \u001b[0;36mTrainer._train_top2vec\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    273\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTop2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nr_topics:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/top2vec/Top2Vec.py:526\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[0;34m(self, documents, min_count, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[1;32m    524\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating joint document/word embedding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc2vec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDoc2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc2vec_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mget_normed_vectors()\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/doc2vec.py:296\u001b[0m, in \u001b[0;36mDoc2Vec.__init__\u001b[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv\u001b[38;5;241m.\u001b[39mvectors_lockf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)  \u001b[38;5;66;03m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoc2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnull_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdm_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:427\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/doc2vec.py:516\u001b[0m, in \u001b[0;36mDoc2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets\n\u001b[1;32m    514\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_doctags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m start_doctags\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoc2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1070\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1076\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1077\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1431\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1431\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m~/BERTopic_evaluation/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1286\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1283\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1286\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=None,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257aee6b-de40-450d-a186-3fd23062804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 20ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe45388-1b9b-48a8-8561-36648f9d7403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [11:59<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset, custom = \"20NewsGroup\", False\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a723341-ae6b-43f7-a1ff-b0b8287a0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt('embeddings_20news.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e351dff1-5947-4a8d-adeb-bf0f010fa767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:30:21,836 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:30:51,394 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:30:51,396 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:30:51,839 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:30:51,841 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:30:52,431 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:30:52,432 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:30:52,956 - BERTopic - Topic reduction - Reduced number of topics from 79 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.08263286838982392\n",
      "diversity: 0.8111111111111111\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:31:00,376 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:31:07,230 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:31:07,232 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:31:07,664 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:31:07,665 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:31:08,212 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:31:08,213 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:31:08,726 - BERTopic - Topic reduction - Reduced number of topics from 86 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1127974268662398\n",
      "diversity: 0.8263157894736842\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:31:16,965 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:31:23,962 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:31:23,964 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:31:24,390 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:31:24,391 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:31:24,979 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:31:24,980 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:31:25,569 - BERTopic - Topic reduction - Reduced number of topics from 83 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1261330586695278\n",
      "diversity: 0.7965517241379311\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:31:35,654 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:31:42,343 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:31:42,344 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:31:42,785 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:31:42,786 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:31:43,434 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:31:43,435 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:31:44,002 - BERTopic - Topic reduction - Reduced number of topics from 91 to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1162164018050954\n",
      "diversity: 0.7871794871794872\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:31:53,640 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:32:00,279 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:32:00,280 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:32:00,715 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:32:00,716 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:32:01,298 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:32:01,299 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:32:01,854 - BERTopic - Topic reduction - Reduced number of topics from 88 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.10983074461215965\n",
      "diversity: 0.773469387755102\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:32:12,256 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:32:18,781 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:32:18,782 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:32:19,204 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:32:19,206 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:32:19,756 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:32:19,757 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:32:20,254 - BERTopic - Topic reduction - Reduced number of topics from 86 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.09540194547165361\n",
      "diversity: 0.8888888888888888\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:32:27,442 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:32:34,527 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:32:34,528 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:32:34,947 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:32:34,948 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:32:35,574 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:32:35,575 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:32:36,131 - BERTopic - Topic reduction - Reduced number of topics from 80 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.11879990875209814\n",
      "diversity: 0.8473684210526315\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:32:44,360 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:32:50,659 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:32:50,661 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:32:51,088 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:32:51,089 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:32:51,643 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:32:51,644 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:32:52,166 - BERTopic - Topic reduction - Reduced number of topics from 88 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1321072355767696\n",
      "diversity: 0.7827586206896552\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:33:00,992 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:33:07,544 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:33:07,545 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:33:07,979 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:33:07,980 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:33:08,535 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:33:08,536 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:33:09,039 - BERTopic - Topic reduction - Reduced number of topics from 87 to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1224636934019986\n",
      "diversity: 0.7538461538461538\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:33:18,916 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:33:25,979 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:33:25,981 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:33:26,411 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:33:26,412 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:33:27,002 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:33:27,003 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:33:27,582 - BERTopic - Topic reduction - Reduced number of topics from 86 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.11338660832436125\n",
      "diversity: 0.7673469387755102\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:33:38,418 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:33:44,683 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:33:44,684 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:33:45,103 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:33:45,104 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:33:45,645 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:33:45,646 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:33:46,137 - BERTopic - Topic reduction - Reduced number of topics from 86 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0933290740563233\n",
      "diversity: 0.9\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:33:53,769 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:34:00,007 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:34:00,008 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:34:00,431 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:34:00,433 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:34:00,956 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:34:00,957 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:34:01,443 - BERTopic - Topic reduction - Reduced number of topics from 81 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.12297255900183968\n",
      "diversity: 0.7894736842105263\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:34:09,644 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:34:16,685 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:34:16,687 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:34:17,131 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:34:17,132 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:34:17,767 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:34:17,768 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:34:18,342 - BERTopic - Topic reduction - Reduced number of topics from 84 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.11598241706452715\n",
      "diversity: 0.8379310344827586\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:34:27,431 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:34:33,950 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:34:33,952 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:34:34,383 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:34:34,384 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:34:34,937 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:34:34,938 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:34:35,460 - BERTopic - Topic reduction - Reduced number of topics from 89 to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1255259139546072\n",
      "diversity: 0.7769230769230769\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:34:44,821 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-03-02 15:34:52,219 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-02 15:34:52,220 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-02 15:34:52,634 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-02 15:34:52,635 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-03-02 15:34:53,178 - BERTopic - Representation - Completed ✓\n",
      "2024-03-02 15:34:53,179 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-03-02 15:34:53,708 - BERTopic - Topic reduction - Reduced number of topics from 82 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.11108401110214756\n",
      "diversity: 0.7510204081632653\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"BERTopic_20news_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e8ad14-d68d-402f-93dd-dc2a1ea38ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:38:19,418 - top2vec - INFO - Pre-processing documents for training\n",
      "2024-03-02 15:38:20,561 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:39:47,484 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:39:54,659 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:39:55,024 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:40:05,129 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1928039352807131\n",
      "diversity: 0.94\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:40:06,301 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:41:34,745 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:41:40,569 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:41:40,933 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:41:52,034 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.19455923591965257\n",
      "diversity: 0.875\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:41:53,294 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:43:21,791 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:43:28,774 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:43:29,126 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:43:40,075 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1943879084523729\n",
      "diversity: 0.82\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:43:41,264 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:45:09,155 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:45:14,553 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:45:14,920 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:45:25,637 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1926203959021126\n",
      "diversity: 0.76\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:45:26,878 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:46:55,912 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:47:02,685 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:47:03,047 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.17497788230343098\n",
      "diversity: 0.694\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:47:13,682 - top2vec - INFO - Pre-processing documents for training\n",
      "2024-03-02 15:47:14,952 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:48:41,867 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:48:47,411 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:48:47,774 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:48:59,495 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.18661965815509793\n",
      "diversity: 0.97\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:49:00,858 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:50:30,122 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:50:37,367 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:50:37,750 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:50:49,497 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.20389153364413284\n",
      "diversity: 0.88\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:50:51,408 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:52:20,367 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:52:25,845 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:52:26,212 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:52:37,387 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.20136939009777138\n",
      "diversity: 0.8066666666666666\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:52:38,614 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:54:09,548 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:54:16,920 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:54:17,286 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:54:27,752 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.18506252137505114\n",
      "diversity: 0.7325\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:54:29,481 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:55:58,024 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:56:04,686 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:56:05,040 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1878169952600734\n",
      "diversity: 0.684\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:56:16,498 - top2vec - INFO - Pre-processing documents for training\n",
      "2024-03-02 15:56:17,714 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:57:49,435 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:57:56,385 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:57:56,794 - top2vec - INFO - Finding topics\n",
      "2024-03-02 15:58:10,029 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.1817414849483188\n",
      "diversity: 0.95\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:58:12,104 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 15:59:48,997 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 15:59:55,901 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 15:59:56,319 - top2vec - INFO - Finding topics\n",
      "2024-03-02 16:00:08,578 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.20697302349940644\n",
      "diversity: 0.91\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 16:00:09,861 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 16:01:38,433 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 16:01:43,870 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 16:01:44,244 - top2vec - INFO - Finding topics\n",
      "2024-03-02 16:01:54,466 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.19505760535703953\n",
      "diversity: 0.8266666666666667\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 16:01:55,720 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 16:03:25,849 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 16:03:31,952 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 16:03:32,308 - top2vec - INFO - Finding topics\n",
      "2024-03-02 16:03:42,625 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.19113874148866128\n",
      "diversity: 0.745\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 16:03:43,806 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-03-02 16:05:12,595 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-03-02 16:05:19,224 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-03-02 16:05:19,573 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.18556954991125615\n",
      "diversity: 0.694\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"20NewsGroup\", False\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=None,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_20news_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87302c78-f969-466f-b4c9-34b9f2bfb542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
