{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8Yuvn6V2zyu3",
   "metadata": {
    "id": "8Yuvn6V2zyu3"
   },
   "source": [
    "# Topic Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VCAel7F_z3WV",
   "metadata": {
    "id": "VCAel7F_z3WV"
   },
   "source": [
    "This notebook contains code needed to run the experiments of my Bachelor's thesis on Topic Modeling Algorithms.\n",
    "\n",
    "For installation please check out `README.md` of this repository if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebd5cb-cdde-427f-adc2-3c0acb10dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5784bd3-6e9b-4698-9cc9-ef3741c4c8b0",
   "metadata": {
    "id": "d5784bd3-6e9b-4698-9cc9-ef3741c4c8b0"
   },
   "outputs": [],
   "source": [
    "# Import neccessary packages\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from evaluation import DataLoader, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CYOXUr_g0mK1",
   "metadata": {
    "id": "CYOXUr_g0mK1"
   },
   "source": [
    "## Trump's tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zk6AyCLl2ENB",
   "metadata": {
    "id": "Zk6AyCLl2ENB"
   },
   "source": [
    "First we will run BERTopic and Top2Vec on Donald Trump's tweets.\n",
    "\n",
    "His tweets' archive can be found here: https://www.thetrumparchive.com/\n",
    "\n",
    "For our experiments we will not consider retweets and deleted tweets, those are being filtered out during the operation of `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leu0zyHf4bfl",
   "metadata": {
    "id": "leu0zyHf4bfl"
   },
   "outputs": [],
   "source": [
    "dataset, custom = \"Trump\", True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d09fac-23e9-4650-8339-3f95f7372d54",
   "metadata": {
    "id": "c6d09fac-23e9-4650-8339-3f95f7372d54",
    "outputId": "cb80de32-ca0a-497a-fadb-b72050a600f3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader_trump = DataLoader(dataset=dataset).\\\n",
    "                    prepare_docs(save=f\"{dataset}.txt\").\\\n",
    "                    preprocess_octis(output_folder=f\"{dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VyIofoXl518p",
   "metadata": {
    "id": "VyIofoXl518p"
   },
   "source": [
    "To speed up BERTopic we precalculate embeddings using `SentenceTransformer all-mpnet-base-v2` model. Otherwise, in every different parameter setting would require calculating embeddings, and that would result in a massive runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f335b-8949-4107-9300-c3b3e9933171",
   "metadata": {
    "id": "630f335b-8949-4107-9300-c3b3e9933171"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "dataset, custom = \"Trump\", True\n",
    "dataloader = DataLoader(dataset)\n",
    "data_trump = dataloader.load_octis(custom)\n",
    "data_trump = [\" \".join(words) for words in data_trump.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings_trump = model.encode(data_trump, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VozHGic187wD",
   "metadata": {
    "id": "VozHGic187wD"
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.savetxt('embeddings_trump.txt', embeddings_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5850a5-fce6-4ac7-b0f4-8b14c0b54e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_trump = np.loadtxt('embeddings_trump.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4RDWEVEH6qS8",
   "metadata": {
    "id": "4RDWEVEH6qS8"
   },
   "source": [
    "As described in the Thesis and the BERTopic paper [https://arxiv.org/pdf/2203.05794.pdf] the performance of the topic models is reflected by two widely-used metrics, topic coherence and topic diversity.\n",
    "\n",
    "For each topic model, coherence is evaluated using Normalized Pointwise Mutual Information (NPMI). This measure ranges from $-1$ to $1$, where $1$ indicates a perfect association.\n",
    "\n",
    "Topic diversity is the percentage of unique words for all topics. This measure ranges from $0$ to $1$ where $0$ indicates redundant topic and $1$ indicates varied topic.\n",
    "\n",
    "Ranging from $10$ to $50$ topics with steps of $10$, the NPMI score is calculated at each step for each topic model. Results are averaged across $3$ runs for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42243fab-8d6a-4cfb-a7a5-bf9d85a3b9f5",
   "metadata": {
    "id": "42243fab-8d6a-4cfb-a7a5-bf9d85a3b9f5"
   },
   "outputs": [],
   "source": [
    "# Evaluating BERTopic on Trump's tweets\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings_trump,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"../results/Basic/Trump/bertopic_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed93cc-38d8-45bc-8777-ae8f52e98e05",
   "metadata": {
    "id": "d0ed93cc-38d8-45bc-8777-ae8f52e98e05"
   },
   "outputs": [],
   "source": [
    "# Evaluating Top2Vec on Trump's tweets\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=None,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"../results/Basic/Trump/Top2Vec_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kEHCgSL-8gNq",
   "metadata": {
    "id": "kEHCgSL-8gNq"
   },
   "source": [
    "## 20Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0T0t6sJ_Vdi",
   "metadata": {
    "id": "j0T0t6sJ_Vdi"
   },
   "source": [
    "The 20Newsgroups dataset comprises 16309 newsgroups posts on 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe45388-1b9b-48a8-8561-36648f9d7403",
   "metadata": {
    "id": "cfe45388-1b9b-48a8-8561-36648f9d7403"
   },
   "outputs": [],
   "source": [
    "dataset, custom = \"20NewsGroup\", False\n",
    "dataloader_20ng = DataLoader(dataset)\n",
    "data_20ng = dataloader_20ng.load_octis(custom)\n",
    "data_20ng = [\" \".join(words) for words in data_20ng.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "#model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "#embeddings_20ng = model.encode(data_20ng, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a723341-ae6b-43f7-a1ff-b0b8287a0fa7",
   "metadata": {
    "id": "3a723341-ae6b-43f7-a1ff-b0b8287a0fa7"
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.savetxt('embeddings_20ng.txt', embeddings_20ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351dff1-5947-4a8d-adeb-bf0f010fa767",
   "metadata": {
    "id": "e351dff1-5947-4a8d-adeb-bf0f010fa767"
   },
   "outputs": [],
   "source": [
    "# Evaluating BERTopic on 20Newsgroups\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings_20ng,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"../results/Basic/20NewsGroups/bertopic_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8ad14-d68d-402f-93dd-dc2a1ea38ba5",
   "metadata": {
    "id": "46e8ad14-d68d-402f-93dd-dc2a1ea38ba5"
   },
   "outputs": [],
   "source": [
    "# Evaluating Top2Vec on 20NewsGroups\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=None,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"../results/Basic/20NewsGroups/Top2Vec_{i+1}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
